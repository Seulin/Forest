{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=1 (NAME), string='aa', start=(1, 0), end=(1, 2), line='aa = 60\\n')\n",
      "TokenInfo(type=53 (OP), string='=', start=(1, 3), end=(1, 4), line='aa = 60\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='60', start=(1, 5), end=(1, 7), line='aa = 60\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 7), end=(1, 8), line='aa = 60\\n')\n",
      "TokenInfo(type=1 (NAME), string='bb', start=(2, 0), end=(2, 2), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string='=', start=(2, 3), end=(2, 4), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(2, 5), end=(2, 6), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='if', start=(2, 7), end=(2, 9), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string='(', start=(2, 10), end=(2, 11), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='aa', start=(2, 11), end=(2, 13), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string='<=', start=(2, 14), end=(2, 16), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='0', start=(2, 17), end=(2, 18), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string=')', start=(2, 18), end=(2, 19), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='else', start=(2, 20), end=(2, 24), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='aa', start=(2, 25), end=(2, 27), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(2, 27), end=(2, 28), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='print', start=(3, 0), end=(3, 5), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=53 (OP), string='(', start=(3, 5), end=(3, 6), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=3 (STRING), string=\"f'{aa} {bb}'\", start=(3, 6), end=(3, 18), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=53 (OP), string=')', start=(3, 18), end=(3, 19), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=55 (COMMENT), string='# aa=0 bb=1', start=(3, 21), end=(3, 32), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(3, 32), end=(3, 33), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=56 (NL), string='\\n', start=(4, 0), end=(4, 1), line='\\n')\n",
      "TokenInfo(type=56 (NL), string='\\n', start=(5, 0), end=(5, 1), line='\\n')\n",
      "TokenInfo(type=1 (NAME), string='if', start=(6, 0), end=(6, 2), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=53 (OP), string='(', start=(6, 2), end=(6, 3), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=1 (NAME), string='aa', start=(6, 3), end=(6, 5), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=53 (OP), string='==', start=(6, 6), end=(6, 8), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='0', start=(6, 9), end=(6, 10), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=53 (OP), string=')', start=(6, 10), end=(6, 11), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=53 (OP), string=':', start=(6, 11), end=(6, 12), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(6, 12), end=(6, 13), line='if(aa == 0):\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(7, 0), end=(7, 4), line='    print()\\n')\n",
      "TokenInfo(type=1 (NAME), string='print', start=(7, 4), end=(7, 9), line='    print()\\n')\n",
      "TokenInfo(type=53 (OP), string='(', start=(7, 9), end=(7, 10), line='    print()\\n')\n",
      "TokenInfo(type=53 (OP), string=')', start=(7, 10), end=(7, 11), line='    print()\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(7, 11), end=(7, 12), line='    print()\\n')\n",
      "TokenInfo(type=56 (NL), string='\\n', start=(8, 0), end=(8, 1), line='\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(9, 0), end=(9, 0), line='aa = 70\\n')\n",
      "TokenInfo(type=1 (NAME), string='aa', start=(9, 0), end=(9, 2), line='aa = 70\\n')\n",
      "TokenInfo(type=53 (OP), string='=', start=(9, 3), end=(9, 4), line='aa = 70\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='70', start=(9, 5), end=(9, 7), line='aa = 70\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(9, 7), end=(9, 8), line='aa = 70\\n')\n",
      "TokenInfo(type=56 (NL), string='\\n', start=(10, 0), end=(10, 1), line='\\n')\n",
      "TokenInfo(type=1 (NAME), string='bb', start=(11, 0), end=(11, 2), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string='=', start=(11, 3), end=(11, 4), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(11, 5), end=(11, 6), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='if', start=(11, 7), end=(11, 9), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string='(', start=(11, 10), end=(11, 11), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='aa', start=(11, 11), end=(11, 13), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string='<=', start=(11, 14), end=(11, 16), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='0', start=(11, 17), end=(11, 18), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=53 (OP), string=')', start=(11, 18), end=(11, 19), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='else', start=(11, 20), end=(11, 24), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='aa', start=(11, 25), end=(11, 27), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(11, 27), end=(11, 28), line='bb = 1 if (aa <= 0) else aa\\n')\n",
      "TokenInfo(type=1 (NAME), string='print', start=(12, 0), end=(12, 5), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=53 (OP), string='(', start=(12, 5), end=(12, 6), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=3 (STRING), string=\"f'{aa} {bb}'\", start=(12, 6), end=(12, 18), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=53 (OP), string=')', start=(12, 18), end=(12, 19), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=55 (COMMENT), string='# aa=0 bb=1', start=(12, 21), end=(12, 32), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(12, 32), end=(12, 33), line=\"print(f'{aa} {bb}')  # aa=0 bb=1\\n\")\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(13, 0), end=(13, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "\n",
    "with tokenize.open('hello.py') as f:\n",
    "    tokens = tokenize.generate_tokens(f.readline)\n",
    "    for token in tokens:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=57 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')\n",
      "TokenInfo(type=1 (NAME), string='aa', start=(1, 0), end=(1, 2), line='aa =60\\n')\n",
      "TokenInfo(type=53 (OP), string='=', start=(1, 3), end=(1, 4), line='aa =60\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='60', start=(1, 4), end=(1, 6), line='aa =60\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 6), end=(1, 7), line='aa =60\\n')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "\n",
    "with open('bye.py', 'rb') as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    for token in tokens:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING ENCODING 'utf-8'\n",
      "NAME NAME 'name'\n",
      "OP EQUAL '='\n",
      "STRING STRING '\"Kildong\"'\n",
      "COMMENT COMMENT '# 이름'\n",
      "NEWLINE NEWLINE '\\r\\n'\n",
      "NAME NAME 'dept'\n",
      "OP EQUAL '='\n",
      "STRING STRING '\"IT\"'\n",
      "COMMENT COMMENT '# 부서'\n",
      "NEWLINE NEWLINE '\\r\\n'\n",
      "NL NL '\\r\\n'\n",
      "COMMENT COMMENT '# str concatenation'\n",
      "NL NL '\\r\\n'\n",
      "NAME NAME 'print'\n",
      "OP LPAR '('\n",
      "STRING STRING '\"I am \"'\n",
      "OP PLUS '+'\n",
      "NAME NAME 'name'\n",
      "OP PLUS '+'\n",
      "STRING STRING '\". I belong to \"'\n",
      "OP PLUS '+'\n",
      "NAME NAME 'dept'\n",
      "OP PLUS '+'\n",
      "STRING STRING '\" department.\"'\n",
      "OP RPAR ')'\n",
      "NEWLINE NEWLINE '\\r\\n'\n",
      "NL NL '\\r\\n'\n",
      "COMMENT COMMENT '# %-formatting'\n",
      "NL NL '\\r\\n'\n",
      "NAME NAME 'print'\n",
      "OP LPAR '('\n",
      "STRING STRING '\"I am %s. I belong to %s department.\"'\n",
      "OP PERCENT '%'\n",
      "OP LPAR '('\n",
      "NAME NAME 'name'\n",
      "OP COMMA ','\n",
      "NAME NAME 'dept'\n",
      "OP RPAR ')'\n",
      "OP RPAR ')'\n",
      "NEWLINE NEWLINE '\\r\\n'\n",
      "NL NL '\\r\\n'\n",
      "COMMENT COMMENT '# str.format()'\n",
      "NL NL '\\r\\n'\n",
      "NAME NAME 'print'\n",
      "OP LPAR '('\n",
      "STRING STRING '\"I am {}. I belong to {} department.\"'\n",
      "OP DOT '.'\n",
      "NAME NAME 'format'\n",
      "OP LPAR '('\n",
      "NAME NAME 'name'\n",
      "OP COMMA ','\n",
      "NAME NAME 'dept'\n",
      "OP RPAR ')'\n",
      "OP RPAR ')'\n",
      "NEWLINE NEWLINE '\\r\\n'\n",
      "NL NL '\\r\\n'\n",
      "COMMENT COMMENT '# f-strings (Python 3.6부터 지원)'\n",
      "NL NL '\\r\\n'\n",
      "NAME NAME 'print'\n",
      "OP LPAR '('\n",
      "STRING STRING 'f\"I am {name}. I belong to {dept} department.\"'\n",
      "OP RPAR ')'\n",
      "NEWLINE NEWLINE '\\r\\n'\n",
      "ENDMARKER ENDMARKER ''\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "\n",
    "with open('hello.py', 'rb') as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    for tok in tokens:\n",
    "        print(tokenize.tok_name[tok.type], tokenize.tok_name[tok.exact_type], repr(tok.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "\n",
    "\n",
    "# 부등호 선호도(<,>,<=,>=)\n",
    "# parameter : tokenize된 값\n",
    "# return: Array(3) => 0: \"\" 의 개수, 1: '' 의 개수, 2: 총 개수\n",
    "\n",
    "def InequalitySign_Preference(tokens):\n",
    "    Less=[\"<\",\"<=\"]\n",
    "    Greater=[\">\",\">=\"] \n",
    "    result=[0,0,0]\n",
    "    for tok in tokens:\n",
    "        if(tokenize.tok_name[tok.type] == \"OP\"):\n",
    "            if (tok.string in Less):\n",
    "                result[0] += 1\n",
    "            elif(tok.string in Greater):\n",
    "                result[1] += 1\n",
    "    result[2]=result[0]+result[1]\n",
    "    \n",
    "    return result    \n",
    "    \n",
    "\n",
    "with open('hello.py', 'rb') as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    print(InequalitySign_Preference(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "\n",
    "# 빈줄 개수\n",
    "\n",
    "def Spared_Line(tokens):\n",
    "    num = 0\n",
    "    for tok in tokens:\n",
    "        if(tokenize.tok_name[tok.type] == \"NL\"):\n",
    "            num += 1\n",
    "\n",
    "    return num\n",
    "        \n",
    "        \n",
    "with open('hello.py', 'rb') as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    print(Spared_Line(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import tokenize\n",
    "\n",
    "def Total_Line(tokens):\n",
    "    num=0\n",
    "    for tok in tokens:\n",
    "        if(tokenize.tok_name[tok.type] == \"NEWLINE\"):\n",
    "            num += 1\n",
    "\n",
    "    return num\n",
    "\n",
    "with open('hello.py', 'rb') as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    print(Total_Line(tokens))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def OperatorBlank_Preference(tokens):\n",
    "    result = [0, 0, 0]\n",
    "    i=(0,0)\n",
    "    for tok in tokens:\n",
    "        if (tokenize.tok_name[tok.type]==\"OP\"):\n",
    "            i=tok.end\n",
    "        elif (i!=(0,0)):\n",
    "            if (i==tok.start):\n",
    "                result[0]+=1\n",
    "            else:\n",
    "                result[1]+=1\n",
    "            i=(0,0)\n",
    "    result[2]=result[0]+result[1]\n",
    "    return result\n",
    "\n",
    "with open('bye.py', 'rb') as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    print(OperatorBlank_Preference(tokens))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
