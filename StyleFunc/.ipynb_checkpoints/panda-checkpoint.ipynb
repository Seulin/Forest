{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenize\n",
    "import naming as naming\n",
    "import suji as sj\n",
    "from crawling import getText, getOneText\n",
    "import io\n",
    "from time import time\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Feature:\n",
    "    __slots__ = ['name', 'users']\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        users_link = 'http://114.71.103.80/new_db.php?q=select+*+from+users%0D%0Alimit+200%0D%0A%0D%0A'\n",
    "        self.users = getText(users_link, '1')\n",
    "\n",
    "    def make_df(feature_func):\n",
    "        def code_link(user):\n",
    "            before = 'http://114.71.103.80/new_db.php?q=select+*+from+code_submission+c%2C+%28select+id%2C+max%28regtime%29+a+from+code_submission+where+id+%3D'\n",
    "            after = '+group+by+p_no%29+d+where+c.id+%3D+d.id+and+d.a+%3D+c.regtime+'\n",
    "            return before + user + after\n",
    "        pass \n",
    "    \n",
    "    def cleaning(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "class NumberFeature(Feature):\n",
    "    pass\n",
    "\n",
    "class ListFeature(Feature):\n",
    "    pass\n",
    "\n",
    "f = Feature('hi')\n",
    "f.problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_row', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "users_link = 'http://114.71.103.80/new_db.php?q=select+*+from+users%0D%0Alimit+200%0D%0A%0D%0A'\n",
    "def code_link(user):\n",
    "    before = 'http://114.71.103.80/new_db.php?q=select+*+from+code_submission+c%2C+%28select+id%2C+max%28regtime%29+a+from+code_submission+where+id+%3D'\n",
    "    after = '+group+by+p_no%29+d+where+c.id+%3D+d.id+and+d.a+%3D+c.regtime+'\n",
    "    return before + user + after\n",
    "\n",
    "\n",
    "users = getText(users_link, '1')\n",
    "total_num_p = 78\n",
    "     \n",
    "\n",
    "def make_feature_df(feature_func, users, total_num_p = total_num_p):\n",
    "    df = pd.DataFrame(columns=users,index=range(1,total_num_p + 1))\n",
    "    start = time()\n",
    "    for user in users:\n",
    "        num_p = getText(code_link(user),  '2')\n",
    "        num_p = list(map(int, num_p))\n",
    "        problems = getText(code_link(user),  '4')\n",
    "        results=[]\n",
    "        for problem in problems:\n",
    "            stream = io.StringIO(problem)\n",
    "            tokens = tokenize.generate_tokens(stream.readline)\n",
    "            try:\n",
    "                results.append(feature_func(tokens))\n",
    "            except:\n",
    "                results.append(None) # None = error in code\n",
    "\n",
    "        result_i = 0\n",
    "        for i, v in enumerate(num_p):\n",
    "            df.loc[v][user] = results[i]\n",
    "        # NaN = problem isn't submitted\n",
    "    end = time()\n",
    "    print(f'Running time {end - start} sec')\n",
    "    return df\n",
    "\n",
    "def avg_cleaning(df): #suji\n",
    "    df.loc[:,'mean'] = df.mean(axis=1)\n",
    "    df = df.divide(df['mean'], axis = 0)\n",
    "    df = df.drop(['mean'], axis=1)\n",
    "    return df\n",
    "\n",
    "def self_cleaning(df): # yeram\n",
    "    num_p, num_u = df.shape\n",
    "    for i in range(1, num_p + 1):\n",
    "        for j in range(num_u):\n",
    "            lst=df.loc[i][j]\n",
    "            if type(lst) != list:\n",
    "                pass\n",
    "            else:\n",
    "                last_num=len(lst)-1\n",
    "                for k in range(len(lst)):\n",
    "                    if lst[last_num] != 0:\n",
    "                        lst[k]=lst[k]/lst[last_num]\n",
    "                del lst[last_num]\n",
    "                df.loc[i][j]=lst\n",
    "    return df\n",
    "\n",
    "\n",
    "def integrate(dfs, prob, user): #seulin\n",
    "    def check_shape(dfs): # check the shape of all dfs\n",
    "        shape = dfs[0].shape\n",
    "        for df in dfs:\n",
    "            assert(shape == df.shape)\n",
    "    check_shape(dfs)\n",
    "    res = []\n",
    "    for df in dfs:\n",
    "        feature = df.loc[prob][user]\n",
    "        if type(feature) == list:\n",
    "            res.extend(feature)\n",
    "        elif type(feature) in [int, float]:\n",
    "            res.append(feature)\n",
    "        else:\n",
    "            # NaN\n",
    "            pass\n",
    "    return res\n",
    "\n",
    "\n",
    "# get_distribution을 위한 함수, df에서 일부 레벨 별로 df를 추출함\n",
    "def get_problems_users(df, level, users):\n",
    "    return df.loc[(level-1)*10+1:level*10, users]\n",
    "\n",
    "def get_std_users(df, level, users):\n",
    "    stds = []\n",
    "    for u in users:\n",
    "        stds.append(get_std(level, u))\n",
    "    return pd.Series(stds, index=users)\n",
    "\n",
    "def get_std(df, level, user):\n",
    "    def get_problems(df, level, user):\n",
    "        #level 1 : 1~10번 문제, 2: 11~20번 문제\n",
    "        return df.loc[(level-1)*10+1:level*10][user]\n",
    "    series = get_problems(df, level, user)\n",
    "    if series.dtype in [int, float]:\n",
    "        return series.std()\n",
    "    elif series.dtype == object:\n",
    "        lst = series.tolist()\n",
    "        new = []\n",
    "        for prob in lst:\n",
    "            if type(prob) == list:\n",
    "                new.append(prob[0]) #나머지 항들도 같은 std 값임.\n",
    "        new = pd.Series(new)\n",
    "        return new.std()\n",
    "#         res = []\n",
    "#         for i in range(len(lst[0])): # length of each list\n",
    "#             new = pd.Series([prob[i] for prob in lst])\n",
    "#             res.append(new.std())\n",
    "#         return res\n",
    "# 유지도 : 개인별로 10개의 문제 기록을 보고 표준편차를 계산, 표준편차가 작으면 높은 가중치\n",
    "\n",
    "def calculate_plagiarism(origin, new, weights):\n",
    "    assert origin.size == new.size == weights.size\n",
    "    diff = origin - new\n",
    "    diff = diff.abs()\n",
    "    return diff*weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_update(res, new_data, ratio = 0.7):\n",
    "    if len(res) == 0:\n",
    "#     ////////////////////////////////////////////////\n",
    "#     왜 깊은 복사 안되지 df찍어보면 안다\n",
    "        res  =  copy.deepcopy(new_data)\n",
    "#     /////////////////////////////////////////////////\n",
    "        return copy.deepcopy(new_data)\n",
    "    \n",
    "    data_size = len(res)\n",
    "    arr_size = len(res[0])\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        for j in range(arr_size):\n",
    "#             print(new_data[i])\n",
    "            try:\n",
    "                val = (1-ratio) * res[i][j] + ratio * new_data[i][j]\n",
    "                res[i][j] = round(val, 5)\n",
    "                \n",
    "            except:\n",
    "                if new_data[i] == None or np.isnan(new_data[i]):\n",
    "                    print(\"nan, none 발생\")\n",
    "                    break\n",
    "            \n",
    "    return res\n",
    "           \n",
    "    \n",
    "def total_update(data):\n",
    "    data_size = len(df)\n",
    "    res = []\n",
    "    \n",
    "    for i in range(1,data_size+1):\n",
    "        res = one_update(res, data.loc[i])\n",
    "        print(\"i=\",i)\n",
    "        print(res)\n",
    "        \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   201811011  20181167  201811132  201811141\n",
      "0         90        88        100          0\n",
      "1         99        76         99         99\n",
      "2         50        95         50         40\n",
      "3         65        79         65         65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "201811011    0.75\n",
       "20181167     0.75\n",
       "201811132    0.75\n",
       "201811141    0.25\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_distribution(data, n_clustering = 2):\n",
    "    \n",
    "    mean_df = data.mean()\n",
    "    np_mean_Df = mean_df.to_numpy().reshape(-1, 1)\n",
    "#     print(np_mean_Df)\n",
    "    \n",
    "    model = KMeans(n_clusters=n_clustering, init='k-means++', n_init=10, max_iter=300, tol=1e-4)\n",
    "    model.fit(np_mean_Df)\n",
    "\n",
    "    y_predict = model.labels_\n",
    "    n_y_predict = len(y_predict)\n",
    "\n",
    "    kmean_result = np.zeros(n_clustering) \n",
    "    \n",
    "    for i in range(n_y_predict):\n",
    "        kmean_result[y_predict[i]]+=1\n",
    "    kmean_result /= n_y_predict\n",
    "    \n",
    "    result = [kmean_result[pred_y] for pred_y in y_predict]\n",
    "    \n",
    "    \n",
    "    return pd.Series(result, index=list(data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = {\n",
    "    '201811011': [90, 99, 50, 65],\n",
    "    '20181167': [88, 76, 95, 79],\n",
    "    '201811132' : [100, 99, 50, 65],\n",
    "    '201811141': [0, 99, 40, 65]\n",
    "}\n",
    "\n",
    "# print(data)\n",
    "df1 = pd.DataFrame(data)\n",
    "print(df1)\n",
    "get_distribution(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time 530.3547825813293 sec\n"
     ]
    }
   ],
   "source": [
    "# 정제한 df들 만들기\n",
    "\n",
    "# for func in [sj. ~~~~] 함수들 자기 전에 돌리고 자자.\n",
    "df1 = make_feature_df(sj.Annotation, users)\n",
    "df1 = self_cleaning(df1)\n",
    "df1.to_pickle(\"./dfs/Annotation.pkl\")\n",
    "\n",
    "df2 = make_feature_df(sj.OpenParOpBl, users)\n",
    "df2 = self_cleaning(df2)\n",
    "df2.to_pickle(\"./dfs/OpenParOpBl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 df 불러오기\n",
    "df1 = pd.read_pickle(\"./dfs/Annotation.pkl\")\n",
    "df2 = pd.read_pickle(\"./dfs/OpenParOpBl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2를 element df로 쪼개는 함수가 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature 가중치 구하기\n",
    "level = 1\n",
    "\n",
    "# kmeans으로 학생들 사이의 분산 구하기\n",
    "temp = get_problems_users(df1, 1, users)\n",
    "print(get_distribution(temp))\n",
    "\n",
    "# 표편으로 유지도 구하기\n",
    "temp = get_std(df1. level, users)\n",
    "print(temp)\n",
    "####################################\n",
    "\n",
    "# kmeans으로 학생들 사이의 분산 구하기\n",
    "temp = get_problems_users(df2, 1, users)\n",
    "print(get_distribution(temp))\n",
    "\n",
    "# 표편으로 유지도 구하기\n",
    "temp = get_std(df2. level, users)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 1.0, 1.153846153846154]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로 들어오는 문제의 코딩 스타일을 정의\n",
    "integrate([df1, df2], 10, \"201411023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and read\n",
    "df.to_pickle(\"./dfs/feature_df.pkl\")\n",
    "df_data = pd.read_pickle(\"./dfs/feature_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
